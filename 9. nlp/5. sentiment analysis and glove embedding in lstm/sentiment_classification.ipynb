{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/amazon_cells_labelled.txt','data/imdb_labelled.txt','data/yelp_labelled.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'data':[],'label':[]})\n",
    "for f in files:\n",
    "    frame = pd.read_csv(f,sep='\\t',names=['data','label'])\n",
    "    df = pd.concat([df,frame],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data  label\n",
       "0    So there is no way for me to plug it in here i...    0.0\n",
       "1                          Good case, Excellent value.    1.0\n",
       "2                               Great for the jawbone.    1.0\n",
       "3    Tied to charger for conversations lasting more...    0.0\n",
       "4                                    The mic is great.    1.0\n",
       "..                                                 ...    ...\n",
       "995  I think food should have flavor and texture an...    0.0\n",
       "996                           Appetite instantly gone.    0.0\n",
       "997  Overall I was not impressed and would not go b...    0.0\n",
       "998  The whole experience was underwhelming, and I ...    0.0\n",
       "999  Then, as if I hadn't wasted enough of my life ...    0.0\n",
       "\n",
       "[2748 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(df['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.texts_to_sequences(df['data'])\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X = pad_sequences(X,padding='post',maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_embedding_matrix(file,word_index,embedding_dim):\n",
    "    vocab_size=len(word_index)+1\n",
    "    embedding_matrix=np.zeros((vocab_size,embedding_dim))\n",
    "    with open(file,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx]=np.array(vector)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix('data/glove.6B.50d.txt',t.word_index,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()\n",
    "model = Sequential(\n",
    "[\n",
    "    Embedding(input_dim =vocab_size,weights=[embedding_matrix],output_dim=50,input_length=max_len),\n",
    "    LSTM(10,activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(32, activation='tanh'),\n",
    "    BatchNormalization(),    \n",
    "    Dense(1, activation='sigmoid')\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(learning_rate=0.001), metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 4s 120ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.4861\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.6751 - accuracy: 0.5840 - val_loss: 0.6890 - val_accuracy: 0.6679\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.5796 - accuracy: 0.7051 - val_loss: 0.7096 - val_accuracy: 0.5139\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.5185 - accuracy: 0.7748 - val_loss: 0.6814 - val_accuracy: 0.5200\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.4652 - accuracy: 0.8050 - val_loss: 0.6838 - val_accuracy: 0.5200\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.4183 - accuracy: 0.8279 - val_loss: 0.6682 - val_accuracy: 0.5588\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.3701 - accuracy: 0.8534 - val_loss: 0.6496 - val_accuracy: 0.7430\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.3415 - accuracy: 0.8575 - val_loss: 0.6460 - val_accuracy: 0.7539\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.2901 - accuracy: 0.8778 - val_loss: 0.6238 - val_accuracy: 0.7515\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.2621 - accuracy: 0.9033 - val_loss: 0.6101 - val_accuracy: 0.7612\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.2428 - accuracy: 0.9033 - val_loss: 0.6083 - val_accuracy: 0.7200\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 1s 56ms/step - loss: 0.2111 - accuracy: 0.9199 - val_loss: 0.5977 - val_accuracy: 0.6921\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.1886 - accuracy: 0.9340 - val_loss: 0.5676 - val_accuracy: 0.7624\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.1681 - accuracy: 0.9366 - val_loss: 0.6528 - val_accuracy: 0.5721\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.1419 - accuracy: 0.9542 - val_loss: 0.5932 - val_accuracy: 0.6655\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.1291 - accuracy: 0.9568 - val_loss: 0.5447 - val_accuracy: 0.7442\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 0.1169 - accuracy: 0.9615 - val_loss: 0.5347 - val_accuracy: 0.7588\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.1026 - accuracy: 0.9730 - val_loss: 0.5185 - val_accuracy: 0.7636\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0907 - accuracy: 0.9761 - val_loss: 0.7720 - val_accuracy: 0.5661\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0811 - accuracy: 0.9823 - val_loss: 0.5376 - val_accuracy: 0.7394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x192659c46d8>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,validation_split=0.3, epochs=20,steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
