{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download a specific package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ''\n",
    "with open('file.txt', 'r',encoding='utf8') as f:\n",
    "    for text in f:\n",
    "        texts += ' ' +text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skip to content\n",
      " Guru99\n",
      " Home\n",
      " Testing\n",
      " SAP\n",
      " Web\n",
      " Must Learn\n",
      " Big Data\n",
      " Live Project\n",
      " AI\n",
      " Blog\n",
      " \n",
      " NLTK Tokenize: Words and Sentences Tokenizer with Example\n",
      " Daniel Johnson\n",
      " By\n",
      " Daniel Johnson\n",
      " Updated\n",
      " March 8, 2022\n",
      " \n",
      " What is Tokenization?\n",
      "\n",
      " Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens.\n",
      "These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization.\n",
      "Tokenization also helps to substitute sensitive data elements with non-sensitive data elements.\n",
      "\n",
      " \n",
      " Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc.\n",
      "It becomes vital to understand the pattern in the text to achieve the above-stated purpose.\n",
      "\n",
      " \n",
      " For the time being, don’t worry about stemming and lemmatization but treat them as steps for textual data cleaning using NLP (Natural language processing).\n",
      "We will discuss stemming and lemmatization later in the tutorial.\n",
      "Tasks such as Text classification or spam filtering makes use of NLP along with deep learning libraries such as Keras and Tensorflow.\n",
      "\n",
      " \n",
      " Natural Language toolkit has very important module NLTK tokenize sentences which further comprises of sub-modules\n",
      " \n",
      " word tokenize\n",
      " sentence tokenize\n",
      " Tokenization of words\n",
      " We use the method word_tokenize() to split a sentence into words.\n",
      "The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications.\n",
      "It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming.\n",
      "Machine learning models need numeric data to be trained and make a prediction.\n",
      "Word tokenization becomes a crucial part of the text (string) to numeric data conversion.\n",
      "Please read about Bag of Words or CountVectorizer.\n",
      "Please refer to below word tokenize NLTK example to understand the theory better.\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " from nltk.tokenize import word_tokenize\n",
      " text =\n",
      "\"God is Great!\n",
      "I won a lottery.\"\n",
      "\n",
      " print(word_tokenize(text))\n",
      "\n",
      " \n",
      " Output: ['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n",
      " \n",
      " Code Explanation\n",
      " word_tokenize module is imported from the NLTK library.\n",
      "\n",
      " A variable “text” is initialized with two sentences.\n",
      "\n",
      " Text variable is passed in word_tokenize module and printed the result.\n",
      "This module breaks each word with punctuation which you can see in the output.\n",
      "\n",
      " Tokenization of Sentences\n",
      " Sub-module available for the above is sent_tokenize.\n",
      "An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization.\n",
      "Imagine you need to count average words per sentence, how you will calculate?\n",
      "For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio.\n",
      "Such output serves as an important feature for machine training as the answer would be numeric.\n",
      "\n",
      " \n",
      " Check the below NLTK tokenizer example to learn how sentence tokenization is different from words tokenization.\n",
      "\n",
      " \n",
      " from nltk.tokenize import sent_tokenize\n",
      " text =\n",
      "\"God is Great!\n",
      "I won a lottery.\"\n",
      " print(sent_tokenize(text))\n",
      "\n",
      " \n",
      " Output: ['God is Great!', 'I won a lottery ']\n",
      "\n",
      " We have 12 words and two sentences for the same input.\n",
      "\n",
      " \n",
      " \n",
      " Explanation of the program:\n",
      " In a line like the previous program, imported the sent_tokenize module.\n",
      "\n",
      " We have taken the same sentence.\n",
      "Further sentence tokenizer in NLTK module parsed that sentences and show output.\n",
      "It is clear that this function breaks each sentence.\n",
      "\n",
      " Above word tokenizer Python examples are good settings stones to understand the mechanics of the word and sentence tokenization.\n",
      "\n",
      " \n",
      " Summary\n",
      " Tokenization in NLP is the process by which a large quantity of text is divided into smaller parts called tokens.\n",
      "\n",
      " Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc.\n",
      "\n",
      " Natural Language toolkit has very important module NLTK tokenize sentence which further comprises of sub-modules\n",
      " We use the method word_tokenize() to split a sentence into words.\n",
      "The output of word tokenizer in NLTK can be converted to Data Frame for better text understanding in machine learning applications.\n",
      "\n",
      " Sub-module available for the above is sent_tokenize.\n",
      "Sentence tokenizer in Python NLTK is an important feature for machine training.\n",
      "\n",
      " You Might Like:\n",
      " How to Download & Install NLTK on Windows/Mac\n",
      " POS Tagging with NLTK and Chunking in NLP [EXAMPLES]\n",
      " Stemming and Lemmatization in Python NLTK with Examples\n",
      " Seq2seq (Sequence to Sequence) Model with PyTorch\n",
      " Natural Language Processing Tutorial: What is NLP?\n",
      "Examples\n",
      " Post navigation\n",
      " Report a Bug\n",
      " Prev\n",
      " Next\n",
      " \n",
      " \n",
      " \n",
      " Top Tutorials\n",
      " About\n",
      " About Us\n",
      " Advertise with Us\n",
      " Write For Us\n",
      " Contact Us\n",
      " Python\n",
      " Testing\n",
      " Hacking\n",
      " Career Suggestion\n",
      " SAP Career Suggestion Tool\n",
      " Software Testing as a Career\n",
      " \n",
      " Interesting\n",
      " eBook\n",
      " Blog\n",
      " Quiz\n",
      " SAP eBook\n",
      " SAP\n",
      " Java\n",
      " SQL\n",
      " Execute online\n",
      " Execute Java Online\n",
      " Execute Javascript\n",
      " Execute HTML\n",
      " Execute Python\n",
      " Selenium\n",
      " Informatica\n",
      " JIRA\n",
      " © Copyright - Guru99 2022         Privacy Policy  |  Affiliate Disclaimer  |  ToS\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sen in doc.sents:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using transformer to tokenize sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664723c3da154bafa9dfba9c13ffe48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e015aacc971422d962729afc87d04ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] skip to content guru99 home testing sap web must learn big data live project ai blog nltk tokenize : words and sentences tokenizer with example daniel johnson by daniel johnson updated march 8, 2022 what is tokenization? tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. these tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization. tokenization also helps to substitute sensitive data elements with non - sensitive data elements. natural language processing is used for building applications such as text classification, intelligent chatbot, sentimental analysis, language translation, etc. it becomes vital to understand the pattern in the text to achieve the above - stated purpose. for the time being, don ’ t worry about stemming and lemmatization but treat them as steps for textual data cleaning using nlp ( natural language processing ). we will discuss stemming and lemmatization later in the tutorial. tasks such as text classification or spam filtering makes use of nlp along with deep learning libraries such as keras and tensorflow. natural language toolkit has very important module nltk tokenize sentences which further comprises of sub - modules word tokenize sentence tokenize tokenization of words we use the method word _ tokenize ( ) to split a sentence into words. the output of word tokenization can be converted to data frame for better text understanding in machine learning applications. it can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming. machine learning models need numeric data to be trained and make a prediction. word tokenization becomes a crucial part of the text ( string ) to numeric data conversion. please read about bag of words or countvectorizer. please refer to below word tokenize nltk example to understand the theory better. from nltk. tokenize import word _ tokenize text = \" god is great! i won a lottery. \" print ( word _ tokenize ( text ) ) output : [\\'god \\',\\'is \\',\\'great \\', \\'! \\',\\'i \\',\\'won \\',\\'a \\',\\'lottery \\', \\'.\\'] code explanation word _ tokenize module is imported from the nltk library. a variable “ text ” is initialized with two sentences. text variable is passed in word _ tokenize module and printed the result. this module breaks each word with punctuation which you can see in the output. tokenization of sentences sub - module available for the above is sent _ tokenize. an obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. imagine you need to count average words per sentence, how you will calculate? for accomplishing such a task, you need both nltk sentence tokenizer as well as nltk word tokenizer to calculate the ratio. such output serves as an important feature for machine training as the answer would be numeric. check the below nltk tokenizer example to learn how sentence tokenization is different from words tokenization. from nltk. tokenize import sent _ tokenize text = \" god is great! i won a lottery. \" print ( sent _ tokenize ( text ) ) output : [\\'god is great! \\',\\'i won a lottery\\'] we have 12 words and two sentences for the same input. explanation of the program : in a line like the previous program, imported the sent _ tokenize module. we have taken the same sentence. further sentence tokenizer in nltk module parsed that sentences and show output. it is clear that this function breaks each sentence. above word tokenizer python examples are good settings stones to understand the mechanics of the word and sentence tokenization. summary tokenization in nlp is the process by which a large quantity of text is divided into smaller parts called tokens. natural language processing is used for building applications such as text classification, intelligent chatbot, sentimental analysis, language translation, etc. natural language toolkit has very important module nltk tokenize sentence which further comprises of sub - modules we use the method word _ tokenize ( ) to split a sentence into words. the output of word tokenizer in nltk can be converted to data frame for better text understanding in machine learning applications. sub - module available for the above is sent _ tokenize. sentence tokenizer in python nltk is an important feature for machine training. you might like : how to download & install nltk on windows / mac pos tagging with nltk and chunking in nlp [ examples ] stemming and lemmatization in python nltk with examples seq2seq ( sequence to sequence ) model with pytorch natural language processing tutorial : what is nlp? examples post navigation report a bug prev next top tutorials about about us advertise with us write for us contact us python testing hacking career suggestion sap career suggestion tool software testing as a career interesting ebook blog quiz sap ebook sap java sql execute online execute java online execute javascript execute html execute python selenium informatica jira © copyright - guru99 2022 privacy policy | affiliate disclaimer | tos [SEP]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip => skip\n",
      "to => to\n",
      "content => content\n",
      "Guru99 => guru99\n",
      "Home => home\n",
      "Testing => test\n",
      "SAP => sap\n",
      "Web => web\n",
      "Must => must\n",
      "Learn => learn\n",
      "Big => big\n",
      "Data => data\n",
      "Live => live\n",
      "Project => project\n",
      "AI => AI\n",
      "Blog => blog\n",
      "NLTK => nltk\n",
      "Tokenize => token\n",
      ": => :\n",
      "Words => word\n",
      "and => and\n",
      "Sentences => sentenc\n",
      "Tokenizer => token\n",
      "with => with\n",
      "Example => exampl\n",
      "Daniel => daniel\n",
      "Johnson => johnson\n",
      "By => By\n",
      "Daniel => daniel\n",
      "Johnson => johnson\n",
      "Updated => updat\n",
      "March => march\n",
      "8 => 8\n",
      ", => ,\n",
      "2022 => 2022\n",
      "What => what\n",
      "is => is\n",
      "Tokenization => token\n",
      "? => ?\n",
      "Tokenization => token\n",
      "is => is\n",
      "the => the\n",
      "process => process\n",
      "by => by\n",
      "which => which\n",
      "a => a\n",
      "large => larg\n",
      "quantity => quantiti\n",
      "of => of\n",
      "text => text\n",
      "is => is\n",
      "divided => divid\n",
      "into => into\n",
      "smaller => smaller\n",
      "parts => part\n",
      "called => call\n",
      "tokens => token\n",
      ". => .\n",
      "These => these\n",
      "tokens => token\n",
      "are => are\n",
      "very => veri\n",
      "useful => use\n",
      "for => for\n",
      "finding => find\n",
      "patterns => pattern\n",
      "and => and\n",
      "are => are\n",
      "considered => consid\n",
      "as => as\n",
      "a => a\n",
      "base => base\n",
      "step => step\n",
      "for => for\n",
      "stemming => stem\n",
      "and => and\n",
      "lemmatization => lemmat\n",
      ". => .\n",
      "Tokenization => token\n",
      "also => also\n",
      "helps => help\n",
      "to => to\n",
      "substitute => substitut\n",
      "sensitive => sensit\n",
      "data => data\n",
      "elements => element\n",
      "with => with\n",
      "non-sensitive => non-sensit\n",
      "data => data\n",
      "elements => element\n",
      ". => .\n",
      "Natural => natur\n",
      "language => languag\n",
      "processing => process\n",
      "is => is\n",
      "used => use\n",
      "for => for\n",
      "building => build\n",
      "applications => applic\n",
      "such => such\n",
      "as => as\n",
      "Text => text\n",
      "classification => classif\n",
      ", => ,\n",
      "intelligent => intellig\n",
      "chatbot => chatbot\n",
      ", => ,\n",
      "sentimental => sentiment\n",
      "analysis => analysi\n",
      ", => ,\n",
      "language => languag\n",
      "translation => translat\n",
      ", => ,\n",
      "etc => etc\n",
      ". => .\n",
      "It => It\n",
      "becomes => becom\n",
      "vital => vital\n",
      "to => to\n",
      "understand => understand\n",
      "the => the\n",
      "pattern => pattern\n",
      "in => in\n",
      "the => the\n",
      "text => text\n",
      "to => to\n",
      "achieve => achiev\n",
      "the => the\n",
      "above-stated => above-st\n",
      "purpose => purpos\n",
      ". => .\n",
      "For => for\n",
      "the => the\n",
      "time => time\n",
      "being => be\n",
      ", => ,\n",
      "don => don\n",
      "’ => ’\n",
      "t => t\n",
      "worry => worri\n",
      "about => about\n",
      "stemming => stem\n",
      "and => and\n",
      "lemmatization => lemmat\n",
      "but => but\n",
      "treat => treat\n",
      "them => them\n",
      "as => as\n",
      "steps => step\n",
      "for => for\n",
      "textual => textual\n",
      "data => data\n",
      "cleaning => clean\n",
      "using => use\n",
      "NLP => nlp\n",
      "( => (\n",
      "Natural => natur\n",
      "language => languag\n",
      "processing => process\n",
      ") => )\n",
      ". => .\n",
      "We => We\n",
      "will => will\n",
      "discuss => discuss\n",
      "stemming => stem\n",
      "and => and\n",
      "lemmatization => lemmat\n",
      "later => later\n",
      "in => in\n",
      "the => the\n",
      "tutorial => tutori\n",
      ". => .\n",
      "Tasks => task\n",
      "such => such\n",
      "as => as\n",
      "Text => text\n",
      "classification => classif\n",
      "or => or\n",
      "spam => spam\n",
      "filtering => filter\n",
      "makes => make\n",
      "use => use\n",
      "of => of\n",
      "NLP => nlp\n",
      "along => along\n",
      "with => with\n",
      "deep => deep\n",
      "learning => learn\n",
      "libraries => librari\n",
      "such => such\n",
      "as => as\n",
      "Keras => kera\n",
      "and => and\n",
      "Tensorflow => tensorflow\n",
      ". => .\n",
      "Natural => natur\n",
      "Language => languag\n",
      "toolkit => toolkit\n",
      "has => ha\n",
      "very => veri\n",
      "important => import\n",
      "module => modul\n",
      "NLTK => nltk\n",
      "tokenize => token\n",
      "sentences => sentenc\n",
      "which => which\n",
      "further => further\n",
      "comprises => compris\n",
      "of => of\n",
      "sub-modules => sub-modul\n",
      "word => word\n",
      "tokenize => token\n",
      "sentence => sentenc\n",
      "tokenize => token\n",
      "Tokenization => token\n",
      "of => of\n",
      "words => word\n",
      "We => We\n",
      "use => use\n",
      "the => the\n",
      "method => method\n",
      "word_tokenize => word_token\n",
      "( => (\n",
      ") => )\n",
      "to => to\n",
      "split => split\n",
      "a => a\n",
      "sentence => sentenc\n",
      "into => into\n",
      "words => word\n",
      ". => .\n",
      "The => the\n",
      "output => output\n",
      "of => of\n",
      "word => word\n",
      "tokenization => token\n",
      "can => can\n",
      "be => be\n",
      "converted => convert\n",
      "to => to\n",
      "Data => data\n",
      "Frame => frame\n",
      "for => for\n",
      "better => better\n",
      "text => text\n",
      "understanding => understand\n",
      "in => in\n",
      "machine => machin\n",
      "learning => learn\n",
      "applications => applic\n",
      ". => .\n",
      "It => It\n",
      "can => can\n",
      "also => also\n",
      "be => be\n",
      "provided => provid\n",
      "as => as\n",
      "input => input\n",
      "for => for\n",
      "further => further\n",
      "text => text\n",
      "cleaning => clean\n",
      "steps => step\n",
      "such => such\n",
      "as => as\n",
      "punctuation => punctuat\n",
      "removal => remov\n",
      ", => ,\n",
      "numeric => numer\n",
      "character => charact\n",
      "removal => remov\n",
      "or => or\n",
      "stemming => stem\n",
      ". => .\n",
      "Machine => machin\n",
      "learning => learn\n",
      "models => model\n",
      "need => need\n",
      "numeric => numer\n",
      "data => data\n",
      "to => to\n",
      "be => be\n",
      "trained => train\n",
      "and => and\n",
      "make => make\n",
      "a => a\n",
      "prediction => predict\n",
      ". => .\n",
      "Word => word\n",
      "tokenization => token\n",
      "becomes => becom\n",
      "a => a\n",
      "crucial => crucial\n",
      "part => part\n",
      "of => of\n",
      "the => the\n",
      "text => text\n",
      "( => (\n",
      "string => string\n",
      ") => )\n",
      "to => to\n",
      "numeric => numer\n",
      "data => data\n",
      "conversion => convers\n",
      ". => .\n",
      "Please => pleas\n",
      "read => read\n",
      "about => about\n",
      "Bag => bag\n",
      "of => of\n",
      "Words => word\n",
      "or => or\n",
      "CountVectorizer => countvector\n",
      ". => .\n",
      "Please => pleas\n",
      "refer => refer\n",
      "to => to\n",
      "below => below\n",
      "word => word\n",
      "tokenize => token\n",
      "NLTK => nltk\n",
      "example => exampl\n",
      "to => to\n",
      "understand => understand\n",
      "the => the\n",
      "theory => theori\n",
      "better => better\n",
      ". => .\n",
      "from => from\n",
      "nltk.tokenize => nltk.token\n",
      "import => import\n",
      "word_tokenize => word_token\n",
      "text => text\n",
      "= => =\n",
      "`` => ``\n",
      "God => god\n",
      "is => is\n",
      "Great => great\n",
      "! => !\n",
      "I => I\n",
      "won => won\n",
      "a => a\n",
      "lottery => lotteri\n",
      ". => .\n",
      "'' => ''\n",
      "print => print\n",
      "( => (\n",
      "word_tokenize => word_token\n",
      "( => (\n",
      "text => text\n",
      ") => )\n",
      ") => )\n",
      "Output => output\n",
      ": => :\n",
      "[ => [\n",
      "'God => 'god\n",
      "' => '\n",
      ", => ,\n",
      "'is => 'i\n",
      "' => '\n",
      ", => ,\n",
      "'Great => 'great\n",
      "' => '\n",
      ", => ,\n",
      "' => '\n",
      "! => !\n",
      "' => '\n",
      ", => ,\n",
      "' => '\n",
      "I => I\n",
      "' => '\n",
      ", => ,\n",
      "'won => 'won\n",
      "' => '\n",
      ", => ,\n",
      "' => '\n",
      "a => a\n",
      "' => '\n",
      ", => ,\n",
      "'lottery => 'lotteri\n",
      "' => '\n",
      ", => ,\n",
      "' => '\n",
      ". => .\n",
      "' => '\n",
      "] => ]\n",
      "Code => code\n",
      "Explanation => explan\n",
      "word_tokenize => word_token\n",
      "module => modul\n",
      "is => is\n",
      "imported => import\n",
      "from => from\n",
      "the => the\n",
      "NLTK => nltk\n",
      "library => librari\n",
      ". => .\n",
      "A => A\n",
      "variable => variabl\n",
      "“ => “\n",
      "text => text\n",
      "” => ”\n",
      "is => is\n",
      "initialized => initi\n",
      "with => with\n",
      "two => two\n",
      "sentences => sentenc\n",
      ". => .\n",
      "Text => text\n",
      "variable => variabl\n",
      "is => is\n",
      "passed => pass\n",
      "in => in\n",
      "word_tokenize => word_token\n",
      "module => modul\n",
      "and => and\n",
      "printed => print\n",
      "the => the\n",
      "result => result\n",
      ". => .\n",
      "This => thi\n",
      "module => modul\n",
      "breaks => break\n",
      "each => each\n",
      "word => word\n",
      "with => with\n",
      "punctuation => punctuat\n",
      "which => which\n",
      "you => you\n",
      "can => can\n",
      "see => see\n",
      "in => in\n",
      "the => the\n",
      "output => output\n",
      ". => .\n",
      "Tokenization => token\n",
      "of => of\n",
      "Sentences => sentenc\n",
      "Sub-module => sub-modul\n",
      "available => avail\n",
      "for => for\n",
      "the => the\n",
      "above => abov\n",
      "is => is\n",
      "sent_tokenize => sent_token\n",
      ". => .\n",
      "An => An\n",
      "obvious => obviou\n",
      "question => question\n",
      "in => in\n",
      "your => your\n",
      "mind => mind\n",
      "would => would\n",
      "be => be\n",
      "why => whi\n",
      "sentence => sentenc\n",
      "tokenization => token\n",
      "is => is\n",
      "needed => need\n",
      "when => when\n",
      "we => we\n",
      "have => have\n",
      "the => the\n",
      "option => option\n",
      "of => of\n",
      "word => word\n",
      "tokenization => token\n",
      ". => .\n",
      "Imagine => imagin\n",
      "you => you\n",
      "need => need\n",
      "to => to\n",
      "count => count\n",
      "average => averag\n",
      "words => word\n",
      "per => per\n",
      "sentence => sentenc\n",
      ", => ,\n",
      "how => how\n",
      "you => you\n",
      "will => will\n",
      "calculate => calcul\n",
      "? => ?\n",
      "For => for\n",
      "accomplishing => accomplish\n",
      "such => such\n",
      "a => a\n",
      "task => task\n",
      ", => ,\n",
      "you => you\n",
      "need => need\n",
      "both => both\n",
      "NLTK => nltk\n",
      "sentence => sentenc\n",
      "tokenizer => token\n",
      "as => as\n",
      "well => well\n",
      "as => as\n",
      "NLTK => nltk\n",
      "word => word\n",
      "tokenizer => token\n",
      "to => to\n",
      "calculate => calcul\n",
      "the => the\n",
      "ratio => ratio\n",
      ". => .\n",
      "Such => such\n",
      "output => output\n",
      "serves => serv\n",
      "as => as\n",
      "an => an\n",
      "important => import\n",
      "feature => featur\n",
      "for => for\n",
      "machine => machin\n",
      "training => train\n",
      "as => as\n",
      "the => the\n",
      "answer => answer\n",
      "would => would\n",
      "be => be\n",
      "numeric => numer\n",
      ". => .\n",
      "Check => check\n",
      "the => the\n",
      "below => below\n",
      "NLTK => nltk\n",
      "tokenizer => token\n",
      "example => exampl\n",
      "to => to\n",
      "learn => learn\n",
      "how => how\n",
      "sentence => sentenc\n",
      "tokenization => token\n",
      "is => is\n",
      "different => differ\n",
      "from => from\n",
      "words => word\n",
      "tokenization => token\n",
      ". => .\n",
      "from => from\n",
      "nltk.tokenize => nltk.token\n",
      "import => import\n",
      "sent_tokenize => sent_token\n",
      "text => text\n",
      "= => =\n",
      "`` => ``\n",
      "God => god\n",
      "is => is\n",
      "Great => great\n",
      "! => !\n",
      "I => I\n",
      "won => won\n",
      "a => a\n",
      "lottery => lotteri\n",
      ". => .\n",
      "'' => ''\n",
      "print => print\n",
      "( => (\n",
      "sent_tokenize => sent_token\n",
      "( => (\n",
      "text => text\n",
      ") => )\n",
      ") => )\n",
      "Output => output\n",
      ": => :\n",
      "[ => [\n",
      "'God => 'god\n",
      "is => is\n",
      "Great => great\n",
      "! => !\n",
      "' => '\n",
      ", => ,\n",
      "' => '\n",
      "I => I\n",
      "won => won\n",
      "a => a\n",
      "lottery => lotteri\n",
      "' => '\n",
      "] => ]\n",
      "We => We\n",
      "have => have\n",
      "12 => 12\n",
      "words => word\n",
      "and => and\n",
      "two => two\n",
      "sentences => sentenc\n",
      "for => for\n",
      "the => the\n",
      "same => same\n",
      "input => input\n",
      ". => .\n",
      "Explanation => explan\n",
      "of => of\n",
      "the => the\n",
      "program => program\n",
      ": => :\n",
      "In => In\n",
      "a => a\n",
      "line => line\n",
      "like => like\n",
      "the => the\n",
      "previous => previou\n",
      "program => program\n",
      ", => ,\n",
      "imported => import\n",
      "the => the\n",
      "sent_tokenize => sent_token\n",
      "module => modul\n",
      ". => .\n",
      "We => We\n",
      "have => have\n",
      "taken => taken\n",
      "the => the\n",
      "same => same\n",
      "sentence => sentenc\n",
      ". => .\n",
      "Further => further\n",
      "sentence => sentenc\n",
      "tokenizer => token\n",
      "in => in\n",
      "NLTK => nltk\n",
      "module => modul\n",
      "parsed => pars\n",
      "that => that\n",
      "sentences => sentenc\n",
      "and => and\n",
      "show => show\n",
      "output => output\n",
      ". => .\n",
      "It => It\n",
      "is => is\n",
      "clear => clear\n",
      "that => that\n",
      "this => thi\n",
      "function => function\n",
      "breaks => break\n",
      "each => each\n",
      "sentence => sentenc\n",
      ". => .\n",
      "Above => abov\n",
      "word => word\n",
      "tokenizer => token\n",
      "Python => python\n",
      "examples => exampl\n",
      "are => are\n",
      "good => good\n",
      "settings => set\n",
      "stones => stone\n",
      "to => to\n",
      "understand => understand\n",
      "the => the\n",
      "mechanics => mechan\n",
      "of => of\n",
      "the => the\n",
      "word => word\n",
      "and => and\n",
      "sentence => sentenc\n",
      "tokenization => token\n",
      ". => .\n",
      "Summary => summari\n",
      "Tokenization => token\n",
      "in => in\n",
      "NLP => nlp\n",
      "is => is\n",
      "the => the\n",
      "process => process\n",
      "by => by\n",
      "which => which\n",
      "a => a\n",
      "large => larg\n",
      "quantity => quantiti\n",
      "of => of\n",
      "text => text\n",
      "is => is\n",
      "divided => divid\n",
      "into => into\n",
      "smaller => smaller\n",
      "parts => part\n",
      "called => call\n",
      "tokens => token\n",
      ". => .\n",
      "Natural => natur\n",
      "language => languag\n",
      "processing => process\n",
      "is => is\n",
      "used => use\n",
      "for => for\n",
      "building => build\n",
      "applications => applic\n",
      "such => such\n",
      "as => as\n",
      "Text => text\n",
      "classification => classif\n",
      ", => ,\n",
      "intelligent => intellig\n",
      "chatbot => chatbot\n",
      ", => ,\n",
      "sentimental => sentiment\n",
      "analysis => analysi\n",
      ", => ,\n",
      "language => languag\n",
      "translation => translat\n",
      ", => ,\n",
      "etc => etc\n",
      ". => .\n",
      "Natural => natur\n",
      "Language => languag\n",
      "toolkit => toolkit\n",
      "has => ha\n",
      "very => veri\n",
      "important => import\n",
      "module => modul\n",
      "NLTK => nltk\n",
      "tokenize => token\n",
      "sentence => sentenc\n",
      "which => which\n",
      "further => further\n",
      "comprises => compris\n",
      "of => of\n",
      "sub-modules => sub-modul\n",
      "We => We\n",
      "use => use\n",
      "the => the\n",
      "method => method\n",
      "word_tokenize => word_token\n",
      "( => (\n",
      ") => )\n",
      "to => to\n",
      "split => split\n",
      "a => a\n",
      "sentence => sentenc\n",
      "into => into\n",
      "words => word\n",
      ". => .\n",
      "The => the\n",
      "output => output\n",
      "of => of\n",
      "word => word\n",
      "tokenizer => token\n",
      "in => in\n",
      "NLTK => nltk\n",
      "can => can\n",
      "be => be\n",
      "converted => convert\n",
      "to => to\n",
      "Data => data\n",
      "Frame => frame\n",
      "for => for\n",
      "better => better\n",
      "text => text\n",
      "understanding => understand\n",
      "in => in\n",
      "machine => machin\n",
      "learning => learn\n",
      "applications => applic\n",
      ". => .\n",
      "Sub-module => sub-modul\n",
      "available => avail\n",
      "for => for\n",
      "the => the\n",
      "above => abov\n",
      "is => is\n",
      "sent_tokenize => sent_token\n",
      ". => .\n",
      "Sentence => sentenc\n",
      "tokenizer => token\n",
      "in => in\n",
      "Python => python\n",
      "NLTK => nltk\n",
      "is => is\n",
      "an => an\n",
      "important => import\n",
      "feature => featur\n",
      "for => for\n",
      "machine => machin\n",
      "training => train\n",
      ". => .\n",
      "You => you\n",
      "Might => might\n",
      "Like => like\n",
      ": => :\n",
      "How => how\n",
      "to => to\n",
      "Download => download\n",
      "& => &\n",
      "Install => instal\n",
      "NLTK => nltk\n",
      "on => on\n",
      "Windows/Mac => windows/mac\n",
      "POS => po\n",
      "Tagging => tag\n",
      "with => with\n",
      "NLTK => nltk\n",
      "and => and\n",
      "Chunking => chunk\n",
      "in => in\n",
      "NLP => nlp\n",
      "[ => [\n",
      "EXAMPLES => exampl\n",
      "] => ]\n",
      "Stemming => stem\n",
      "and => and\n",
      "Lemmatization => lemmat\n",
      "in => in\n",
      "Python => python\n",
      "NLTK => nltk\n",
      "with => with\n",
      "Examples => exampl\n",
      "Seq2seq => seq2seq\n",
      "( => (\n",
      "Sequence => sequenc\n",
      "to => to\n",
      "Sequence => sequenc\n",
      ") => )\n",
      "Model => model\n",
      "with => with\n",
      "PyTorch => pytorch\n",
      "Natural => natur\n",
      "Language => languag\n",
      "Processing => process\n",
      "Tutorial => tutori\n",
      ": => :\n",
      "What => what\n",
      "is => is\n",
      "NLP => nlp\n",
      "? => ?\n",
      "Examples => exampl\n",
      "Post => post\n",
      "navigation => navig\n",
      "Report => report\n",
      "a => a\n",
      "Bug => bug\n",
      "Prev => prev\n",
      "Next => next\n",
      "Top => top\n",
      "Tutorials => tutori\n",
      "About => about\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About => about\n",
      "Us => Us\n",
      "Advertise => advertis\n",
      "with => with\n",
      "Us => Us\n",
      "Write => write\n",
      "For => for\n",
      "Us => Us\n",
      "Contact => contact\n",
      "Us => Us\n",
      "Python => python\n",
      "Testing => test\n",
      "Hacking => hack\n",
      "Career => career\n",
      "Suggestion => suggest\n",
      "SAP => sap\n",
      "Career => career\n",
      "Suggestion => suggest\n",
      "Tool => tool\n",
      "Software => softwar\n",
      "Testing => test\n",
      "as => as\n",
      "a => a\n",
      "Career => career\n",
      "Interesting => interest\n",
      "eBook => ebook\n",
      "Blog => blog\n",
      "Quiz => quiz\n",
      "SAP => sap\n",
      "eBook => ebook\n",
      "SAP => sap\n",
      "Java => java\n",
      "SQL => sql\n",
      "Execute => execut\n",
      "online => onlin\n",
      "Execute => execut\n",
      "Java => java\n",
      "Online => onlin\n",
      "Execute => execut\n",
      "Javascript => javascript\n",
      "Execute => execut\n",
      "HTML => html\n",
      "Execute => execut\n",
      "Python => python\n",
      "Selenium => selenium\n",
      "Informatica => informatica\n",
      "JIRA => jira\n",
      "© => ©\n",
      "Copyright => copyright\n",
      "- => -\n",
      "Guru99 => guru99\n",
      "2022 => 2022\n",
      "Privacy => privaci\n",
      "Policy => polici\n",
      "| => |\n",
      "Affiliate => affili\n",
      "Disclaimer => disclaim\n",
      "| => |\n",
      "ToS => to\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w + ' => ' + ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting nouns from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip\n",
      "Guru99\n",
      "Home\n",
      "Testing\n",
      "SAP\n",
      "Web\n",
      "Big\n",
      "Data\n",
      "Live\n",
      "Project\n",
      "AI\n",
      "Blog\n",
      "NLTK\n",
      "Tokenize\n",
      "Words\n",
      "Sentences\n",
      "Tokenizer\n",
      "Example\n",
      "Daniel\n",
      "Johnson\n",
      "Daniel\n",
      "Johnson\n",
      "March\n",
      "Tokenization\n",
      "Tokenization\n",
      "process\n",
      "quantity\n",
      "text\n",
      "parts\n",
      "tokens\n",
      "tokens\n",
      "patterns\n",
      "base\n",
      "step\n",
      "stemming\n",
      "lemmatization\n",
      "Tokenization\n",
      "data\n",
      "elements\n",
      "data\n",
      "elements\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "building\n",
      "applications\n",
      "Text\n",
      "classification\n",
      "chatbot\n",
      "analysis\n",
      "language\n",
      "translation\n",
      "pattern\n",
      "text\n",
      "purpose\n",
      "time\n",
      "being\n",
      "stemming\n",
      "lemmatization\n",
      "steps\n",
      "data\n",
      "cleaning\n",
      "NLP\n",
      "language\n",
      "processing\n",
      "lemmatization\n",
      "tutorial\n",
      "Tasks\n",
      "Text\n",
      "classification\n",
      "filtering\n",
      "use\n",
      "NLP\n",
      "learning\n",
      "libraries\n",
      "Keras\n",
      "Tensorflow\n",
      "Natural\n",
      "Language\n",
      "toolkit\n",
      "module\n",
      "NLTK\n",
      "sentences\n",
      "comprises\n",
      "word\n",
      "sentence\n",
      "tokenize\n",
      "Tokenization\n",
      "words\n",
      "method\n",
      "word_tokenize\n",
      "sentence\n",
      "words\n",
      "output\n",
      "word\n",
      "tokenization\n",
      "Data\n",
      "Frame\n",
      "text\n",
      "understanding\n",
      "machine\n",
      "learning\n",
      "applications\n",
      "input\n",
      "text\n",
      "cleaning\n",
      "steps\n",
      "punctuation\n",
      "removal\n",
      "numeric\n",
      "character\n",
      "removal\n",
      "stemming\n",
      "Machine\n",
      "learning\n",
      "models\n",
      "data\n",
      "prediction\n",
      "Word\n",
      "tokenization\n",
      "part\n",
      "text\n",
      "string\n",
      "data\n",
      "conversion\n",
      "Bag\n",
      "Words\n",
      "CountVectorizer\n",
      "word\n",
      "NLTK\n",
      "example\n",
      "theory\n",
      "nltk.tokenize\n",
      "import\n",
      "word_tokenize\n",
      "text\n",
      "God\n",
      "lottery\n",
      "print(word_tokenize(text\n",
      "Output\n",
      "God\n",
      "'\n",
      "lottery\n",
      "Code\n",
      "Explanation\n",
      "word_tokenize\n",
      "module\n",
      "NLTK\n",
      "library\n",
      "text\n",
      "sentences\n",
      "Text\n",
      "variable\n",
      "word_tokenize\n",
      "module\n",
      "result\n",
      "module\n",
      "word\n",
      "punctuation\n",
      "output\n",
      "Tokenization\n",
      "Sentences\n",
      "Sub\n",
      "-\n",
      "module\n",
      "above\n",
      "question\n",
      "mind\n",
      "sentence\n",
      "tokenization\n",
      "option\n",
      "word\n",
      "tokenization\n",
      "words\n",
      "sentence\n",
      "task\n",
      "NLTK\n",
      "sentence\n",
      "tokenizer\n",
      "NLTK\n",
      "word\n",
      "tokenizer\n",
      "ratio\n",
      "output\n",
      "feature\n",
      "machine\n",
      "training\n",
      "answer\n",
      "NLTK\n",
      "tokenizer\n",
      "example\n",
      "sentence\n",
      "tokenization\n",
      "words\n",
      "tokenization\n",
      "nltk.tokenize\n",
      "import\n",
      "sent_tokenize\n",
      "text\n",
      "God\n",
      "lottery\n",
      "Output\n",
      "God\n",
      "lottery\n",
      "words\n",
      "sentences\n",
      "input\n",
      "Explanation\n",
      "program\n",
      "line\n",
      "program\n",
      "sent_tokenize\n",
      "module\n",
      "sentence\n",
      "sentence\n",
      "tokenizer\n",
      "NLTK\n",
      "module\n",
      "sentences\n",
      "output\n",
      "function\n",
      "sentence\n",
      "word\n",
      "tokenizer\n",
      "Python\n",
      "examples\n",
      "settings\n",
      "stones\n",
      "mechanics\n",
      "word\n",
      "sentence\n",
      "tokenization\n",
      "Summary\n",
      "Tokenization\n",
      "NLP\n",
      "process\n",
      "quantity\n",
      "text\n",
      "parts\n",
      "tokens\n",
      "language\n",
      "processing\n",
      "building\n",
      "applications\n",
      "Text\n",
      "classification\n",
      "chatbot\n",
      "analysis\n",
      "language\n",
      "translation\n",
      "Natural\n",
      "Language\n",
      "toolkit\n",
      "module\n",
      "NLTK\n",
      "sentence\n",
      "comprises\n",
      "sub\n",
      "-\n",
      "modules\n",
      "method\n",
      "word_tokenize\n",
      "sentence\n",
      "words\n",
      "output\n",
      "word\n",
      "tokenizer\n",
      "NLTK\n",
      "Data\n",
      "Frame\n",
      "text\n",
      "understanding\n",
      "machine\n",
      "learning\n",
      "applications\n",
      "Sub\n",
      "-\n",
      "module\n",
      "above\n",
      "Sentence\n",
      "tokenizer\n",
      "Python\n",
      "NLTK\n",
      "feature\n",
      "machine\n",
      "training\n",
      "Download\n",
      "Install\n",
      "NLTK\n",
      "Windows\n",
      "Mac\n",
      "POS\n",
      "Tagging\n",
      "NLTK\n",
      "NLP\n",
      "EXAMPLES\n",
      "Stemming\n",
      "Lemmatization\n",
      "Python\n",
      "NLTK\n",
      "Examples\n",
      "Seq2seq\n",
      "Sequence\n",
      "Sequence\n",
      "Model\n",
      "PyTorch\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "Tutorial\n",
      "NLP\n",
      "Examples\n",
      "navigation\n",
      "Bug\n",
      "Prev\n",
      "Next\n",
      "Top\n",
      "Tutorials\n",
      "Us\n",
      "Advertise\n",
      "Us\n",
      "Write\n",
      "Us\n",
      "Contact\n",
      "Us\n",
      "Python\n",
      "Testing\n",
      "Hacking\n",
      "Career\n",
      "Suggestion\n",
      "SAP\n",
      "Career\n",
      "Suggestion\n",
      "Tool\n",
      "Software\n",
      "Testing\n",
      "Career\n",
      "Interesting\n",
      "eBook\n",
      "Blog\n",
      "Quiz\n",
      "SAP\n",
      "eBook\n",
      "SAP\n",
      "Java\n",
      "SQL\n",
      "Execute\n",
      "Java\n",
      "Online\n",
      "Execute\n",
      "Javascript\n",
      "Execute\n",
      "HTML\n",
      "Python\n",
      "Selenium\n",
      "Informatica\n",
      "JIRA\n",
      "©\n",
      "Copyright\n",
      "Guru99\n",
      "Privacy\n",
      "Policy\n",
      "|\n",
      "Affiliate\n",
      "Disclaimer\n",
      "|\n",
      "ToS\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
